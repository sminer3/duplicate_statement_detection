{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "docdb = pd.read_csv(\"../../raw/documentdb_matches.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the changes that has taken place with the GroupSolver platform is the use of implicit matching. Implicit matching as we define it is when a respondent, in the elaboration phase, agrees with one statement and disagrees with another. Since the respondent is showing differing agreement between the statements, we count that as an 'UNEQUAL' evaluation. There was also a bug reported for a certain time where we used a less strict criteria where if a respondent agreed or disagreed to a statement, and put unsure on the other, we also counted that as an 'UNEQUAL'. The first part of this script will examine the impact to matching that this had."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(docdb.resolution==\"EQUAL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "364088"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(docdb.resolution==\"UNEQUAL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the 'UNEQUAL's come from implicit matching. Let's see how many unequals there are when we limit it to pairs that had at least 2 direct matching evaluations of unequal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11207"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum((docdb.resolution==\"UNEQUAL\") & (docdb.respUnequal + docdb.respNotSure > 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there could be matches where we didn't record it as such since the implicit matching out weighed the actual matches. Let's look at how many matches we would've had if we used the 4/4 or 6/7 rule. We'll use the percentage 6/7 and whether they had at least 4 evaluations as the criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "789"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docdb[\"n_eval\"] = docdb.respEqual + docdb.respUnequal + docdb.respNotSure\n",
    "np.sum((docdb.respEqual / docdb.n_eval > 6/7) & (docdb.n_eval > 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of matches we missed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "527"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum((docdb.respEqual / docdb.n_eval > 6/7) & (docdb.n_eval > 3)) - np.sum(docdb.resolution==\"EQUAL\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
